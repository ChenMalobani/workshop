{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xsKpnea0HQr"
   },
   "source": [
    "# Attention on VGGNet (Saliency and grad-CAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y2tRvRWh0HQy"
   },
   "source": [
    "## Saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wn1RKXWG0HQ2"
   },
   "source": [
    "To visualize activation over final dense layer outputs, we need to switch the `softmax` activation out for `linear` since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide `utils.apply_modifications` to modify network parameters and rebuild the graph.\n",
    "\n",
    "If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjlZ4e5i163b"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/raghakot/keras-vis.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjaS_ACU0HQ8"
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "# Build the VGG16 network with ImageNet weights\n",
    "model1 = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model1, 'predictions')\n",
    "\n",
    "# Swap softmax with linear\n",
    "model1.layers[layer_idx].activation = activations.linear\n",
    "model1 = utils.apply_modifications(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zMJQOqM3vIn"
   },
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "\n",
    "model = Model(model1.input, model1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7v_rcqI0HRT"
   },
   "source": [
    "Lets load a couple of test images to try saliency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CbLny2vk0HRY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from vis.utils import utils\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "img1 = utils.load_img('https://i.ibb.co/tmSywkd/arab.jpg', target_size=(224, 224))\n",
    "img2 = utils.load_img('https://github.com/raghakot/keras-vis/raw/master/examples/vggnet/images/ouzel2.jpg', target_size=(224, 224))\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(img1)\n",
    "ax[1].imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvaG3c1f0HRn"
   },
   "source": [
    "Time for saliency visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyU12xA50HRp"
   },
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_saliency, overlay\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "for i, img in enumerate([img1, img2]):    \n",
    "    # 20 is the imagenet index corresponding to `ouzel`\n",
    "    grads = visualize_saliency(model, layer_idx, filter_indices=None, seed_input=img)\n",
    "    \n",
    "    # visualize grads as heatmap\n",
    "    ax[i].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUYzIPBC0HR1"
   },
   "source": [
    "Not that great. Very noisy. Lets try guided and rectified saliency.\n",
    "\n",
    "To use guided saliency, we need to set `backprop_modifier='guided'`. For rectified saliency or deconv saliency, use `backprop_modifier='relu'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgIHwUMk0HR5"
   },
   "outputs": [],
   "source": [
    "for modifier in ['guided', 'relu']:\n",
    "    plt.figure()\n",
    "    f, ax = plt.subplots(1, 2)\n",
    "    plt.suptitle(modifier)\n",
    "    for i, img in enumerate([img1, img2]):    \n",
    "        # 20 is the imagenet index corresponding to `ouzel`\n",
    "        grads = visualize_saliency(model, layer_idx, filter_indices=[20, 365], \n",
    "                                   seed_input=img, backprop_modifier=modifier)\n",
    "        # Lets overlay the heatmap onto original image.    \n",
    "        ax[i].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_La7g2M0HST"
   },
   "source": [
    "guided saliency is definitely better. I am not sure whats going on with rectified saliency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "si_NHW2X0HSZ"
   },
   "source": [
    "## grad-CAM - vanilla, guided, rectified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dOHrKs90HSe"
   },
   "source": [
    "These should contain more detail since they use `Conv` or `Pooling` features that contain more spatial detail which is lost in `Dense` layers. The only additional detail compared to saliency is the `penultimate_layer_idx`. This specifies the pre-layer whose gradients should be used. See this paper for technical details: https://arxiv.org/pdf/1610.02391v1.pdf\n",
    "\n",
    "By default, if `penultimate_layer_idx` is not defined, it searches for the nearest pre layer. For our architecture, that would be the `block5_pool` layer after all the `Conv` layers. Here is the model summary for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIUrC84M0HSh"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oGhWfjIg0HS4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from vis.visualization import visualize_cam\n",
    "print(np.max(img1),np.max(img2))\n",
    "print(np.argmax(model.predict([[img1]])))\n",
    "for i in range(1):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
    "    for i, img in enumerate([img1]):  \n",
    "        \n",
    "        # 20 is the imagenet index corresponding to `ouzel`\n",
    "        grads = visualize_cam(model, layer_idx, filter_indices=20, \n",
    "                              seed_input=img, backprop_modifier=None)        \n",
    "        grads2 = visualize_cam(model, layer_idx, filter_indices=None, \n",
    "                              seed_input=img, backprop_modifier=None)        \n",
    "        grads = np.maximum(grads,grads2)\n",
    "        # Lets overlay the heatmap onto original image.    \n",
    "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
    "        plt.imshow(overlay(jet_heatmap, img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Er2C_YJX0HTL"
   },
   "source": [
    "guided grad-CAM wins again. It far less noisy than other options."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "attention.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
