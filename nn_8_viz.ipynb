{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nn_8_viz.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5ph6HK4Hw5B",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eyaler/workshop/blob/master/nn_8_viz.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BsBFEmpK4tIe"
      },
      "source": [
        "# Attention on VGGNet (Saliency and grad-CAM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OwZ2XM7p4tIk"
      },
      "source": [
        "## Saliency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8fxgcYqL4tIn"
      },
      "source": [
        "To visualize activation over final dense layer outputs, we need to switch the `softmax` activation out for `linear` since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide `utils.apply_modifications` to modify network parameters and rebuild the graph.\n",
        "\n",
        "If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QC0Z86W14vsa",
        "colab": {}
      },
      "source": [
        "# update keras-vis to master\n",
        "!pip install -U git+git://github.com/raghakot/keras-vis.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6SDI1E94tIp",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "import numpy as np\n",
        "\n",
        "# Build the VGG16 network with ImageNet weights\n",
        "model = VGG16(weights='imagenet', include_top=True)\n",
        "\n",
        "# Utility to search for layer index by name. \n",
        "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
        "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
        "\n",
        "# Swap softmax with linear\n",
        "model.layers[layer_idx].activation = activations.linear\n",
        "model = utils.apply_modifications(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Omy8aZl64tI5"
      },
      "source": [
        "Lets load a couple of test images to try saliency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EIBY8iiO4tI7",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "from vis.utils import utils\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (18, 6)\n",
        "\n",
        "img1 = 'https://raw.githubusercontent.com/raghakot/keras-vis/master/examples/vggnet/images/ouzel1.jpg'\n",
        "img2 = 'https://raw.githubusercontent.com/raghakot/keras-vis/master/examples/vggnet/images/ouzel2.jpg'\n",
        "\n",
        "img1 = utils.load_img(img1, target_size=(224, 224))[..., :3]\n",
        "img2 = utils.load_img(img2, target_size=(224, 224))[..., :3]\n",
        "\n",
        "f, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(img1)\n",
        "ax[1].imshow(img2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n3oGlCez50k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = [np.argmax(model.predict(img1[None,:])), np.argmax(model.predict(img2[None,:]))]\n",
        "print(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HWh8i6F94tJF"
      },
      "source": [
        "Time for saliency visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KRA7BqbP4tJH",
        "colab": {}
      },
      "source": [
        "from vis.visualization import visualize_saliency, overlay\n",
        "from vis.utils import utils\n",
        "from keras import activations\n",
        "\n",
        "# Utility to search for layer index by name. \n",
        "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
        "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
        "\n",
        "f, ax = plt.subplots(1, 2)\n",
        "for i, img in enumerate([img1, img2]):    \n",
        "    # 20 is the imagenet index corresponding to `ouzel`\n",
        "    grads = visualize_saliency(model, layer_idx, filter_indices=index[i], seed_input=img)\n",
        "    \n",
        "    # visualize grads as heatmap\n",
        "    ax[i].imshow(grads, cmap='jet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SBZ4hoKe4tJO"
      },
      "source": [
        "Not that great. Very noisy. Lets try guided and rectified saliency.\n",
        "\n",
        "To use guided saliency, we need to set `backprop_modifier='guided'`. For rectified saliency or deconv saliency, use `backprop_modifier='relu'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9bnPpGDz4tJQ",
        "colab": {}
      },
      "source": [
        "for modifier in ['guided', 'relu']:\n",
        "    plt.figure()\n",
        "    f, ax = plt.subplots(1, 2)\n",
        "    plt.suptitle(modifier)\n",
        "    for i, img in enumerate([img1, img2]):    \n",
        "        # 20 is the imagenet index corresponding to `ouzel`\n",
        "        grads = visualize_saliency(model, layer_idx, filter_indices=index[i], \n",
        "                                   seed_input=img, backprop_modifier=modifier)\n",
        "        # Lets overlay the heatmap onto original image.    \n",
        "        ax[i].imshow(grads, cmap='jet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cz_rRhHaiZ-q",
        "colab": {}
      },
      "source": [
        "  import cv2\n",
        "  import matplotlib.cm as cm\n",
        "  from PIL import Image\n",
        "\n",
        "  quantile=0.99\n",
        "  threshold =0.1\n",
        "  alpha=0.5\n",
        "  \n",
        "  f, ax = plt.subplots(1, 2)\n",
        "  for i, img in enumerate([img1, img2]):  \n",
        "    viz = visualize_saliency(model, layer_idx, filter_indices=index[i], \n",
        "                                 seed_input=img, backprop_modifier='guided')\n",
        "    viz = np.clip(viz,None,np.quantile(viz, quantile))\n",
        "    viz = (viz-np.min(viz))/(np.max(viz)-np.min(viz)+1e-7)\n",
        "    viz = cv2.GaussianBlur(viz, (9,9), 0)\n",
        "\n",
        "    jet_heatmap = cm.jet(viz)[..., :3]\n",
        "    mask = viz[..., np.newaxis]>=threshold\n",
        "\n",
        "    img = np.mean(img, axis=-1, keepdims=True)/255\n",
        "    ax[i].imshow((alpha*jet_heatmap+(1-alpha)*img)*mask+img*(1-mask))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CsG42j8G4tJZ"
      },
      "source": [
        "guided saliency is definitely better. I am not sure whats going on with rectified saliency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mhIlFlxi4tJc"
      },
      "source": [
        "## grad-CAM - vanilla, guided, rectified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L7UcYYlU4tJe"
      },
      "source": [
        "These should contain more detail since they use `Conv` or `Pooling` features that contain more spatial detail which is lost in `Dense` layers. The only additional detail compared to saliency is the `penultimate_layer_idx`. This specifies the pre-layer whose gradients should be used. See this paper for technical details: https://arxiv.org/pdf/1610.02391v1.pdf\n",
        "\n",
        "By default, if `penultimate_layer_idx` is not defined, it searches for the nearest pre layer. For our architecture, that would be the `block5_pool` layer after all the `Conv` layers. Here is the model summary for reference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z0s6fr6K4tJh",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKrhBGo74tJs",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "from vis.visualization import visualize_cam\n",
        "\n",
        "for modifier in [None, 'guided', 'relu']:\n",
        "    plt.figure()\n",
        "    f, ax = plt.subplots(1, 2)\n",
        "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
        "    for i, img in enumerate([img1, img2]):    \n",
        "        # 20 is the imagenet index corresponding to `ouzel`\n",
        "        grads = visualize_cam(model, layer_idx, filter_indices=index[i], \n",
        "                              seed_input=img, backprop_modifier=modifier)        \n",
        "        # Lets overlay the heatmap onto original image.    \n",
        "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
        "        ax[i].imshow(overlay(jet_heatmap, img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WTujrt_Z4tJ3"
      },
      "source": [
        "guided grad-CAM wins again. It far less noisy than other options. Note: in the literature \"guided grad-CAM usually means something else i.e. grad-CAM multiplied by  guided backprop\""
      ]
    }
  ]
}
