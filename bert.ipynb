{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6OYMDBCMWoEF","colab_type":"code","colab":{}},"source":["pip install pytorch_transformers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KyuTQ1VZbtaY","colab_type":"code","colab":{}},"source":["import torch\n","from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n","\n","# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n","#import logging\n","#logging.basicConfig(level=logging.INFO)\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\n","\n","def get_probs(text):\n","  text = '[CLS] '+text.lstrip('[CLS] ').rstrip(' [SEP]')+' [SEP]'\n","  # Tokenize input\n","  tokenized_text = tokenizer.tokenize(text)\n","  #print(tokenized_text)\n","\n","  # Convert token to vocabulary indices\n","  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","  segments_ids = [0]*len(tokenized_text)\n","\n","  # Convert inputs to PyTorch tensors\n","  tokens_tensor = torch.tensor([indexed_tokens])\n","  segments_tensors = torch.tensor([segments_ids])\n","\n","  # Load pre-trained model (weights)\n","  model = BertModel.from_pretrained('bert-base-uncased')\n","\n","  # Set the model in evaluation mode to desactivate the DropOut modules\n","  # This is IMPORTANT to have reproductible results during evaluation!\n","  model.eval()\n","\n","  # If you have a GPU, put everything on cuda\n","  tokens_tensor = tokens_tensor.to('cuda')\n","  segments_tensors = segments_tensors.to('cuda')\n","  model.to('cuda')\n","\n","  # Predict hidden states features for each layer\n","  with torch.no_grad():\n","      # See the models docstrings for the detail of the inputs\n","      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","      # PyTorch-Transformers models always output tuples.\n","      # See the models docstrings for the detail of all the outputs\n","      # In our case, the first element is the hidden state of the last layer of the Bert model\n","      encoded_layers = outputs[0]\n","  # We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n","  assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)\n","\n","  # Load pre-trained model (weights)\n","  model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","  model.eval()\n","\n","  # If you have a GPU, put everything on cuda\n","  tokens_tensor = tokens_tensor.to('cuda')\n","  segments_tensors = segments_tensors.to('cuda')\n","  model.to('cuda')\n","\n","  # Predict all tokens\n","  with torch.no_grad():\n","      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","      predictions = outputs[0]\n","\n","  predicted_probs = [round(torch.softmax(predictions[0, i], 0)[j].item(),4) for i,j in enumerate(indexed_tokens)]\n","  \n","  return list(zip(tokenized_text, predicted_probs))[1:-1]\n","\n","def predict_word(text, topn=10):\n","  text = '[CLS] '+text.lstrip('[CLS] ').rstrip(' [SEP]')+' [SEP]'\n","  # Tokenize input\n","  tokenized_text = tokenizer.tokenize(text)\n","  #print(tokenized_text)\n","\n","  # Mask a token that we will try to predict back with `BertForMaskedLM`\n","  masked_index = -1\n","  for i,token in enumerate(tokenized_text):\n","    if token=='[MASK]':\n","      masked_index = i\n","      break\n","  assert i>=0\n","\n","  # Convert token to vocabulary indices\n","  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","  segments_ids = [0]*len(tokenized_text)\n","\n","  # Convert inputs to PyTorch tensors\n","  tokens_tensor = torch.tensor([indexed_tokens])\n","  segments_tensors = torch.tensor([segments_ids])\n","\n","  # Load pre-trained model (weights)\n","  model = BertModel.from_pretrained('bert-base-uncased')\n","\n","  # Set the model in evaluation mode to desactivate the DropOut modules\n","  # This is IMPORTANT to have reproductible results during evaluation!\n","  model.eval()\n","\n","  # If you have a GPU, put everything on cuda\n","  tokens_tensor = tokens_tensor.to('cuda')\n","  segments_tensors = segments_tensors.to('cuda')\n","  model.to('cuda')\n","\n","  # Predict hidden states features for each layer\n","  with torch.no_grad():\n","      # See the models docstrings for the detail of the inputs\n","      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","      # PyTorch-Transformers models always output tuples.\n","      # See the models docstrings for the detail of all the outputs\n","      # In our case, the first element is the hidden state of the last layer of the Bert model\n","      encoded_layers = outputs[0]\n","  # We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n","  assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)\n","\n","  # Load pre-trained model (weights)\n","  model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","  model.eval()\n","\n","  # If you have a GPU, put everything on cuda\n","  tokens_tensor = tokens_tensor.to('cuda')\n","  segments_tensors = segments_tensors.to('cuda')\n","  model.to('cuda')\n","\n","  # Predict all tokens\n","  with torch.no_grad():\n","      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","      predictions = outputs[0]\n","\n","  predicted_inds = torch.argsort(-predictions[0, masked_index])\n","  predicted_probs = [round(p.item(),4) for p in torch.softmax(predictions[0, masked_index], 0)[predicted_inds]]\n","  predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n","  \n","  return list(zip(predicted_tokens, predicted_probs))[:topn]\n","\n","def fix_word(text):\n","  probs = [p[1] for p in get_probs(text)]\n","  ind = torch.argmin(torch.tensor(probs))\n","  tokenized_text = tokenizer.tokenize(text)\n","  bad_word = tokenized_text[ind]\n","  tokenized_text[ind] = '[MASK]'\n","  fix = predict_word(' '.join(tokenized_text), 1)[0][0]\n","  tokenized_text[ind] = fix\n","  return ' '.join(tokenized_text)\n","  \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3X_j9L7hbxt2","colab_type":"code","colab":{}},"source":["predict_word('The boy [MASK] to the school')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ZQWt5nHpHuQ","colab_type":"code","colab":{}},"source":["predict_word('Alex likes to have [MASK] with his best friend')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wxz1yC4iuO5a","colab_type":"code","colab":{}},"source":["get_probs('The boy want to the school')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQBnd7nKuQaB","colab_type":"code","colab":{}},"source":["fix_word('The boy want to the school')"],"execution_count":0,"outputs":[]}]}