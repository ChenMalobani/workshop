{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "heb_bert.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eyaler/workshop/blob/master/heb_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OYMDBCMWoEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyuTQ1VZbtaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, BertTokenizer, AutoModel, AutoModelWithLMHead, BertModel, BertForMaskedLM\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
        "#import logging\n",
        "#logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\")\n",
        "\n",
        "def get_probs(text):\n",
        "  text = '[CLS] '+text.lstrip('[CLS] ').rstrip(' [SEP]')+' [SEP]'\n",
        "  # Tokenize input\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  #print(tokenized_text)\n",
        "\n",
        "  # Convert token to vocabulary indices\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "  segments_ids = [0]*len(tokenized_text)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Load pre-trained model (weights)\n",
        "  #model = BertModel.from_pretrained('bert-base-uncased')\n",
        "  model = AutoModel.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\")\n",
        "\n",
        "  # Set the model in evaluation mode to desactivate the DropOut modules\n",
        "  # This is IMPORTANT to have reproductible results during evaluation!\n",
        "  model.eval()\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  segments_tensors = segments_tensors.to('cuda')\n",
        "  model.to('cuda')\n",
        "\n",
        "  # Predict hidden states features for each layer\n",
        "  with torch.no_grad():\n",
        "      # See the models docstrings for the detail of the inputs\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "      # PyTorch-Transformers models always output tuples.\n",
        "      # See the models docstrings for the detail of all the outputs\n",
        "      # In our case, the first element is the hidden state of the last layer of the Bert model\n",
        "      encoded_layers = outputs[0]\n",
        "  # We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
        "  assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)\n",
        "\n",
        "  # Load pre-trained model (weights)\n",
        "  #model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "  model = AutoModelWithLMHead.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\")\n",
        "  model.eval()\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  segments_tensors = segments_tensors.to('cuda')\n",
        "  model.to('cuda')\n",
        "\n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "      predictions = outputs[0]\n",
        "\n",
        "  predicted_probs = [round(torch.softmax(predictions[0, i], 0)[j].item(),4) for i,j in enumerate(indexed_tokens)]\n",
        "  \n",
        "  return list(zip(tokenized_text, predicted_probs))[1:-1]\n",
        "\n",
        "def predict_word(text, topn=10):\n",
        "  text = '[CLS] '+text.lstrip('[CLS] ').rstrip(' [SEP]')+' [SEP]'\n",
        "  # Tokenize input\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  #print(tokenized_text)\n",
        "\n",
        "  # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "  masked_index = -1\n",
        "  for i,token in enumerate(tokenized_text):\n",
        "    if token=='[MASK]':\n",
        "      masked_index = i\n",
        "      break\n",
        "  assert i>=0\n",
        "\n",
        "  # Convert token to vocabulary indices\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "  segments_ids = [0]*len(tokenized_text)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Load pre-trained model (weights)\n",
        "  #model = BertModel.from_pretrained('bert-base-uncased')\n",
        "  model = AutoModel.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\")\n",
        "\n",
        "  # Set the model in evaluation mode to desactivate the DropOut modules\n",
        "  # This is IMPORTANT to have reproductible results during evaluation!\n",
        "  model.eval()\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  segments_tensors = segments_tensors.to('cuda')\n",
        "  model.to('cuda')\n",
        "\n",
        "  # Predict hidden states features for each layer\n",
        "  with torch.no_grad():\n",
        "      # See the models docstrings for the detail of the inputs\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "      # PyTorch-Transformers models always output tuples.\n",
        "      # See the models docstrings for the detail of all the outputs\n",
        "      # In our case, the first element is the hidden state of the last layer of the Bert model\n",
        "      encoded_layers = outputs[0]\n",
        "  # We have encoded our input sequence in a FloatTensor of shape (batch size, sequence length, model hidden dimension)\n",
        "  assert tuple(encoded_layers.shape) == (1, len(indexed_tokens), model.config.hidden_size)\n",
        "\n",
        "  # Load pre-trained model (weights)\n",
        "  #model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "  model = AutoModelWithLMHead.from_pretrained(\"TurkuNLP/wikibert-base-he-cased\")\n",
        "  model.eval()\n",
        "\n",
        "  # If you have a GPU, put everything on cuda\n",
        "  tokens_tensor = tokens_tensor.to('cuda')\n",
        "  segments_tensors = segments_tensors.to('cuda')\n",
        "  model.to('cuda')\n",
        "\n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "      predictions = outputs[0]\n",
        "\n",
        "  predicted_inds = torch.argsort(-predictions[0, masked_index])\n",
        "  predicted_probs = [round(p.item(),4) for p in torch.softmax(predictions[0, masked_index], 0)[predicted_inds]]\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n",
        "  \n",
        "  return list(zip(predicted_tokens, predicted_probs))[:topn]\n",
        "\n",
        "def fix_word(text):\n",
        "  probs = [p[1] for p in get_probs(text)]\n",
        "  ind = torch.argmin(torch.tensor(probs))\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  bad_word = tokenized_text[ind]\n",
        "  tokenized_text[ind] = '[MASK]'\n",
        "  fix = predict_word(' '.join(tokenized_text), 1)[0][0]\n",
        "  tokenized_text[ind] = fix\n",
        "  return ' '.join(tokenized_text)\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohhzxxCKgBBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = \"TurkuNLP/wikibert-base-he-cased\"\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "#model = AutoModel.from_pretrained(MODEL_PATH)\n",
        "#model.eval()\n",
        "\n",
        "#tokenizer= BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelWithLMHead.from_pretrained(MODEL_PATH)\n",
        "model.eval()\n",
        "\n",
        "def whatisit(text):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "    print(\"Encode: \" + str(tokens))\n",
        "    print(\"Decode: \" + str(tokenizer.decode(tokens)))\n",
        "    print(\"[PAD]: \" + str(tokenizer.encode(\"[PAD]\")[1]))\n",
        "    print(\"[MASK]: \" + str(tokenizer.encode(\"[MASK]\")[1]))\n",
        "\n",
        "    input_ids = torch.tensor(tokens).unsqueeze(0)  # Batch size 1\n",
        "    outputs = model(input_ids, masked_lm_labels=input_ids)    \n",
        "\n",
        "    loss, prediction_scores = outputs[:2]\n",
        "    \n",
        "    print (\"len(prediction_scores): \" + str(len(prediction_scores)))\n",
        "    print (\"prediction_scores.shape: \" + str(prediction_scores.shape))\n",
        "\n",
        "    return loss, prediction_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_j9L7hbxt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#פרדי מרקורי מאסק זמר ומוזיקאי\n",
        "\n",
        "\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "#print(whatisit(\"זמר ומוזיקאי \" + tokenizer.mask_token + \" פרדי מרקורי\"))\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "print(whatisit(\"פרדי מרקורי [MASK] זמר ומוזיקאי\"))\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "#print(predict_word('זמר ומוזיקאי [MASK] פרדי מרקורי'))\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "print(predict_word('פרדי מרקורי [MASK] זמר ומוזיקאי'))\n",
        "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
        "\n",
        "#פרדי מרקורי היה מאסק ומוזיקאי\n",
        "\n",
        "print(predict_word('פרדי מרקורי היה [MASK] ומוזיקאי'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxz1yC4iuO5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_probs('פרדי מרקורי זמר ומוזיקאי')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQBnd7nKuQaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fix_word('פרדי מרקורי זמר ומוזיקאי')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}