{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eyaler/workshop/blob/master/nn_8_attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BsBFEmpK4tIe"
   },
   "source": [
    "# Attention on VGGNet (Saliency and grad-CAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwZ2XM7p4tIk"
   },
   "source": [
    "## Saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fxgcYqL4tIn"
   },
   "source": [
    "To visualize activation over final dense layer outputs, we need to switch the `softmax` activation out for `linear` since gradient of output node will depend on all the other node activations. Doing this in keras is tricky, so we provide `utils.apply_modifications` to modify network parameters and rebuild the graph.\n",
    "\n",
    "If this swapping is not done, the results might be suboptimal. We will start by swapping out 'softmax' for 'linear'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11792,
     "status": "ok",
     "timestamp": 1557611760659,
     "user": {
      "displayName": "Eyal Gruss",
      "photoUrl": "https://lh4.googleusercontent.com/-u09YaZrQAqY/AAAAAAAAAAI/AAAAAAAAC64/Edn8Phw0CRs/s64/photo.jpg",
      "userId": "10806915125759667398"
     },
     "user_tz": -180
    },
    "id": "QC0Z86W14vsa",
    "outputId": "6f28f26c-e52e-4c80-8d40-aead77815497"
   },
   "outputs": [],
   "source": [
    "!pip install -U git+git://github.com/raghakot/keras-vis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32981,
     "status": "ok",
     "timestamp": 1557611781888,
     "user": {
      "displayName": "Eyal Gruss",
      "photoUrl": "https://lh4.googleusercontent.com/-u09YaZrQAqY/AAAAAAAAAAI/AAAAAAAAC64/Edn8Phw0CRs/s64/photo.jpg",
      "userId": "10806915125759667398"
     },
     "user_tz": -180
    },
    "id": "n6SDI1E94tIp",
    "outputId": "4b17e14a-6760-498b-9077-c4eadb93af85"
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "# Build the VGG16 network with ImageNet weights\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
    "\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = activations.linear\n",
    "model = utils.apply_modifications(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Omy8aZl64tI5"
   },
   "source": [
    "Lets load a couple of test images to try saliency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1891,
     "status": "ok",
     "timestamp": 1557612657824,
     "user": {
      "displayName": "Eyal Gruss",
      "photoUrl": "https://lh4.googleusercontent.com/-u09YaZrQAqY/AAAAAAAAAAI/AAAAAAAAC64/Edn8Phw0CRs/s64/photo.jpg",
      "userId": "10806915125759667398"
     },
     "user_tz": -180
    },
    "id": "EIBY8iiO4tI7",
    "outputId": "81e4b92a-65b2-448a-fb79-5306a5bac98a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from vis.utils import utils\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "img1 = 'https://raw.githubusercontent.com/raghakot/keras-vis/master/examples/vggnet/images/ouzel1.jpg'\n",
    "img2 = 'https://raw.githubusercontent.com/raghakot/keras-vis/master/examples/vggnet/images/ouzel2.jpg'\n",
    "\n",
    "img1 = utils.load_img(img1, target_size=(224, 224))[..., :3]\n",
    "img2 = utils.load_img(img2, target_size=(224, 224))[..., :3]\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(img1)\n",
    "ax[1].imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWh8i6F94tJF"
   },
   "source": [
    "Time for saliency visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6751,
     "status": "ok",
     "timestamp": 1557612667371,
     "user": {
      "displayName": "Eyal Gruss",
      "photoUrl": "https://lh4.googleusercontent.com/-u09YaZrQAqY/AAAAAAAAAAI/AAAAAAAAC64/Edn8Phw0CRs/s64/photo.jpg",
      "userId": "10806915125759667398"
     },
     "user_tz": -180
    },
    "id": "KRA7BqbP4tJH",
    "outputId": "1e1e2520-ecbb-49ec-d80c-62dd86b4c0e1"
   },
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_saliency, overlay\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
    "\n",
    "f, ax = plt.subplots(1, 2)\n",
    "for i, img in enumerate([img1, img2]):    \n",
    "    # 20 is the imagenet index corresponding to `ouzel`\n",
    "    grads = visualize_saliency(model, layer_idx, filter_indices=20, seed_input=img)\n",
    "    \n",
    "    # visualize grads as heatmap\n",
    "    ax[i].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBZ4hoKe4tJO"
   },
   "source": [
    "Not that great. Very noisy. Lets try guided and rectified saliency.\n",
    "\n",
    "To use guided saliency, we need to set `backprop_modifier='guided'`. For rectified saliency or deconv saliency, use `backprop_modifier='relu'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11157,
     "status": "ok",
     "timestamp": 1557612681356,
     "user": {
      "displayName": "Eyal Gruss",
      "photoUrl": "https://lh4.googleusercontent.com/-u09YaZrQAqY/AAAAAAAAAAI/AAAAAAAAC64/Edn8Phw0CRs/s64/photo.jpg",
      "userId": "10806915125759667398"
     },
     "user_tz": -180
    },
    "id": "9bnPpGDz4tJQ",
    "outputId": "3fa752d0-1b04-4d97-a578-375ae981730a"
   },
   "outputs": [],
   "source": [
    "for modifier in ['guided', 'relu']:\n",
    "    plt.figure()\n",
    "    f, ax = plt.subplots(1, 2)\n",
    "    plt.suptitle(modifier)\n",
    "    for i, img in enumerate([img1, img2]):    \n",
    "        # 20 is the imagenet index corresponding to `ouzel`\n",
    "        grads = visualize_saliency(model, layer_idx, filter_indices=20, \n",
    "                                   seed_input=img, backprop_modifier=modifier)\n",
    "        # Lets overlay the heatmap onto original image.    \n",
    "        ax[i].imshow(grads, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cz_rRhHaiZ-q"
   },
   "outputs": [],
   "source": [
    "  img = img1\n",
    "  index = np.argmax(model.predict(img))\n",
    "  grads = visualize_saliency(model, layer_idx, filter_indices=index, \n",
    "                               seed_input=img, backprop_modifier='guided')\n",
    "  viz = np.clip(viz,None,np.quantile(viz, quantile))\n",
    "  viz = (viz-np.min(viz))/(np.max(viz)-np.min(viz)+1e-7)\n",
    "  viz = cv2.GaussianBlur(viz, (9,9), 0)\n",
    "\n",
    "  plt.figure()\n",
    "  jet_heatmap = cm.jet(viz)[..., :3]\n",
    "  mask = viz[..., np.newaxis]>=threshold\n",
    "\n",
    "  img = Image.open(filename)\n",
    "  img = np.asarray(img.resize(viz.shape[::-1], resample=Image.BILINEAR))/255\n",
    "\n",
    "  filename = os.path.basename(filename)\n",
    "  plt.title(filename + ' ' + label + ' ' + str(prob))\n",
    "  plt.imshow((alpha*jet_heatmap+(1-alpha)*img)*mask+img*(1-mask))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsG42j8G4tJZ"
   },
   "source": [
    "guided saliency is definitely better. I am not sure whats going on with rectified saliency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhIlFlxi4tJc"
   },
   "source": [
    "## grad-CAM - vanilla, guided, rectified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7UcYYlU4tJe"
   },
   "source": [
    "These should contain more detail since they use `Conv` or `Pooling` features that contain more spatial detail which is lost in `Dense` layers. The only additional detail compared to saliency is the `penultimate_layer_idx`. This specifies the pre-layer whose gradients should be used. See this paper for technical details: https://arxiv.org/pdf/1610.02391v1.pdf\n",
    "\n",
    "By default, if `penultimate_layer_idx` is not defined, it searches for the nearest pre layer. For our architecture, that would be the `block5_pool` layer after all the `Conv` layers. Here is the model summary for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1043
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1557612688449,
     "user": {
      "displayName": "Eyal Gruss",
      "photoUrl": "https://lh4.googleusercontent.com/-u09YaZrQAqY/AAAAAAAAAAI/AAAAAAAAC64/Edn8Phw0CRs/s64/photo.jpg",
      "userId": "10806915125759667398"
     },
     "user_tz": -180
    },
    "id": "z0s6fr6K4tJh",
    "outputId": "a97d3fef-d77a-4410-bfc0-a70f95b60a82"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4396,
     "status": "ok",
     "timestamp": 1557612695638,
     "user": {
      "displayName": "Eyal Gruss",
      "photoUrl": "https://lh4.googleusercontent.com/-u09YaZrQAqY/AAAAAAAAAAI/AAAAAAAAC64/Edn8Phw0CRs/s64/photo.jpg",
      "userId": "10806915125759667398"
     },
     "user_tz": -180
    },
    "id": "gKrhBGo74tJs",
    "outputId": "52f0fbd1-9276-4890-c857-6dd610582f0e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from vis.visualization import visualize_cam\n",
    "\n",
    "for modifier in [None, 'guided', 'relu']:\n",
    "    plt.figure()\n",
    "    f, ax = plt.subplots(1, 2)\n",
    "    plt.suptitle(\"vanilla\" if modifier is None else modifier)\n",
    "    for i, img in enumerate([img1, img2]):    \n",
    "        # 20 is the imagenet index corresponding to `ouzel`\n",
    "        grads = visualize_cam(model, layer_idx, filter_indices=20, \n",
    "                              seed_input=img, backprop_modifier=modifier)        \n",
    "        # Lets overlay the heatmap onto original image.    \n",
    "        jet_heatmap = np.uint8(cm.jet(grads)[..., :3] * 255)\n",
    "        ax[i].imshow(overlay(jet_heatmap, img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTujrt_Z4tJ3"
   },
   "source": [
    "guided grad-CAM wins again. It far less noisy than other options."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nn_8_attention.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/raghakot/keras-vis/blob/master/examples/vggnet/attention.ipynb",
     "timestamp": 1557611661589
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
